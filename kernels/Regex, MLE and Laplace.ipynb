{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd \n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
    "from nltk.util import bigrams\n",
    "from nltk.lm import MLE,Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex and MLE/Laplace perperplexity\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>idioma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;Here is a piece of C++ code that seems very...</td>\n",
       "      <td>ingles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;I accidentally committed the wrong files to...</td>\n",
       "      <td>ingles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  idioma\n",
       "0  <p>Here is a piece of C++ code that seems very...  ingles\n",
       "1  <p>I accidentally committed the wrong files to...  ingles"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dadosPT = pd.read_csv('../bases/stackoverflow_portugues.csv')\n",
    "dadosPT['idioma'] = 'portugues'\n",
    "dadosEN = pd.read_csv('../bases/stackoverflow_ingles.csv')\n",
    "dadosEN['idioma'] = 'ingles'\n",
    "dadosES = pd.read_csv('../bases/stackoverflow_espanhol.csv')\n",
    "dadosES['idioma'] = 'espanhol'\n",
    "\n",
    "dados = pd.concat([dadosEN,dadosPT,dadosES], ignore_index=True).reset_index(drop=True)[['Questão','idioma']]\n",
    "dados = dados.rename({\"Questão\":'question'},axis=1)\n",
    "dados.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1011    hice un commit e inmediatamente después noté q...\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def removeWords(word,regex,sub=\"\"):\n",
    "    if type(word) == str:\n",
    "        return regex.sub(sub,word)\n",
    "    else:\n",
    "        return [regex.sub(sub,w) for w in word]\n",
    "    \n",
    "regex = re.compile(\"<code(.|(\\n))*?/code>\")\n",
    "#print(removeWords(dadosEN['Questão'][33],regex))\n",
    "# remove everything between code tag with any character or breakrow, and ? is to get the higher amount of words as possible\n",
    "dados['question'] = removeWords(dados['question'],regex)\n",
    "\n",
    "# remove all html tags remaning\n",
    "regex = re.compile(\"<.*?>\")\n",
    "#print(removeWords(dadosEN['Questão'][33],regex))\n",
    "dados['question'] = removeWords(dados['question'],regex)\n",
    "\n",
    "# Drop all except alphanumeric and spaces\n",
    "regex = re.compile(r\"[^\\w\\s]\")\n",
    "#print(removeWords(dadosPT['Questão'][33],regex))\n",
    "dados['question'] = removeWords(dados['question'],regex)\n",
    "\n",
    "# Drop numbers\n",
    "regex = re.compile(\"\\d+\")\n",
    "#print(removeWords(dadosPT['Questão'][33],regex))\n",
    "dados['question'] = removeWords(dados['question'],regex)\n",
    "\n",
    "# spaces and \\n repited or not will be replaced by \" \"\n",
    "regex = re.compile(r\"\\s+\")\n",
    "#print(removeWords(dadosPT['Questão'][33],regex,sub=\" \"))\n",
    "dados['question'] = removeWords(dados['question'],regex,sub=\" \")\n",
    "\n",
    "# Strip blank spaces\n",
    "dados['question'] = dados['question'].str.strip()\n",
    "\n",
    "# let everything to lowercase\n",
    "dados['question'] = dados['question'].str.lower()\n",
    "\n",
    "\n",
    "dados['question'].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    idioma:\n",
    "    {\n",
    "        'train_test': train_test_split(dados[dados['idioma'] == idioma]['question'], test_size=0.2, random_state=42),\n",
    "    }\n",
    " for idioma in dados['idioma'].unique()\n",
    "}\n",
    "\n",
    "models = {\n",
    "    idioma:\n",
    "    {\n",
    "        'train': items['train_test'][0],\n",
    "        'test': items['train_test'][1],\n",
    "    }\n",
    " for idioma,items in models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(train,model):\n",
    "    vocabulary = ' '.join(train)\n",
    "    vocabulary = WhitespaceTokenizer().tokenize(vocabulary)\n",
    "    wordBigrams,vocabulary = padded_everygram_pipeline(2,vocabulary)\n",
    "    model.fit(wordBigrams,vocabulary)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idioma in models.keys():\n",
    "    models[idioma]['modelMLE'] = fit_model(models[idioma]['train'],MLE(2))\n",
    "for idioma in models.keys():\n",
    "    models[idioma]['modelLaplace'] = fit_model(models[idioma]['train'],Laplace(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model,text,tokenizer=WhitespaceTokenizer()):\n",
    "    words = tokenizer.tokenize(text)\n",
    "    fake_chars = [list(pad_both_ends(word, n = 2)) for word in words]\n",
    "    wordBigrams = [list(bigrams(word)) for word in fake_chars]\n",
    "    \n",
    "    return sum([model.perplexity(word) for word in wordBigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.8325426888377\n",
      "33.915936843206005\n"
     ]
    }
   ],
   "source": [
    "print(calculate_perplexity(models['ingles']['modelMLE'], \"good morning\"))\n",
    "print(calculate_perplexity(models['ingles']['modelLaplace'], \"good morning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_argmax(text,use_model='modelMLE'):\n",
    "    perplexities = {\n",
    "        idioma:calculate_perplexity(models[idioma][use_model], text)\n",
    "        for idioma in models.keys()\n",
    "    }\n",
    "    return pd.Series(perplexities).idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'portugues'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_argmax(\"deixa os garoto brinca power ranger \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLE\n",
      "ingles accuracy: 0.98\n",
      "portugues accuracy: 0.8\n",
      "espanhol accuracy: 0.86\n",
      "Using Laplace\n",
      "ingles accuracy: 1.0\n",
      "portugues accuracy: 1.0\n",
      "espanhol accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"Using MLE\")\n",
    "for idioma in models.keys():\n",
    "    predicted = models[idioma]['test'].apply(lambda x: perplexity_argmax(x,use_model='modelMLE'))\n",
    "    print(f\"{idioma} accuracy: {round((predicted == idioma).sum()/predicted.shape[0],2)}\")\n",
    "    \n",
    "print(\"Using Laplace\")\n",
    "for idioma in models.keys():\n",
    "    predicted = models[idioma]['test'].apply(lambda x: perplexity_argmax(x,use_model='modelLaplace'))\n",
    "    print(f\"{idioma} accuracy: {round((predicted == idioma).sum()/predicted.shape[0],2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tengo una serie de en mi vista cada muestra una unidad de medida y en caso de querer editarla se reemplaza via javascript por un via boton nota tanto las labels como los input son elementos con classccc_unidad para iterarlos posteriormente el resultante simplificado quedaria algo asi como véis en el snippet intento coger el valor de la label si no lo tiene busco el valor del input pero no esta funcionando correctamente cómo puedo diferenciar que tipo de elemento es la variable para así coger el atributo correspondiente a cada tipo',\n",
       " 'ingles')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[idioma]['test'][predicted != idioma].sample().values[0],predicted[predicted != idioma].sample().values[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
